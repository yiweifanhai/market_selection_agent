{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb278548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuxin/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import threading\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class HolidayKeywordMiner:\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        data_path: 数据文件路径\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        # 去掉所有列都为-1的行\n",
    "        self.data = self.data[~(self.data.drop(columns=['keyword']).eq(-1).all(axis=1))]\n",
    "        self.keywords = self.data['keyword']\n",
    "        # 提取日期列\n",
    "        date_columns = [col for col in self.data.columns if col != 'keyword']\n",
    "        self.dates = pd.to_datetime(date_columns, format='%Y%m%d')\n",
    "        # 创建时间序列数据\n",
    "        self.ts_data = self.data[date_columns].values\n",
    "        self.ts_data[self.ts_data == -1] = 100000  # 将-1替换为大值\n",
    "        self.config = {\n",
    "            'pre_holiday_start': 60,  # 节前60天\n",
    "            'pre_holiday_end': 3,    # 节日前3天\n",
    "            \n",
    "            'post_holiday_start': 7, # 节后7天\n",
    "            'post_holiday_end': 14, # 节后2周\n",
    "\n",
    "            'peak_window_start': 45, # 节日前45天\n",
    "            'peak_window_end': 7,    # 节日后7天\n",
    "            \n",
    "            'pre_holiday_earlystart': 120,  # 节前90天\n",
    "            'post_holiday_lateend': 30,    # 节后3周\n",
    "        }\n",
    "    \n",
    "    \"\"\"\n",
    "    节日前60天向上，节日前1天到节日后14天向下，且最高峰在节日前45天到节日后7天内\n",
    "    \"\"\"\n",
    "    def define_holiday_periods(self, holiday_date, years=[2023, 2024, 2025]):\n",
    "        \"\"\"\n",
    "        定义节日期间\n",
    "        holiday_date: 节日日期，如 '02-14' (情人节)\n",
    "        years: 要分析的年份列表\n",
    "        \"\"\"\n",
    "        holiday_periods = {}\n",
    "        for year in years:\n",
    "            holiday = f\"{year}-{holiday_date}\"\n",
    "            holiday_dt = pd.to_datetime(holiday)\n",
    "            \n",
    "            # 节日前后时间段定义\n",
    "            pre_holiday_start = holiday_dt - timedelta(days=self.config['pre_holiday_start'])  # 节前60天\n",
    "            pre_holiday_end = holiday_dt - timedelta(days=self.config['pre_holiday_end'])     # 节日前7天\n",
    "\n",
    "            post_holiday_start = holiday_dt - timedelta(days=self.config['post_holiday_start'])  # 节日前7天\n",
    "            post_holiday_end = holiday_dt + timedelta(days=self.config['post_holiday_end'])   # 节后2周\n",
    "            \n",
    "            peak_window_start = holiday_dt - timedelta(days=self.config['peak_window_start'])  # 节日前45天\n",
    "            peak_window_end = holiday_dt + timedelta(days=self.config['peak_window_end'])    # 节日后7天\n",
    "            \n",
    "            # 用于计算启动期、波峰期、衰退期的索引范围\n",
    "            pre_holiday_earlystart = holiday_dt - timedelta(days=self.config['pre_holiday_earlystart'])  # 节前90天\n",
    "            post_holiday_lateend = holiday_dt + timedelta(days=self.config['post_holiday_lateend'])  # 节后3周\n",
    "            \n",
    "            holiday_periods[year] = {\n",
    "                'holiday_date': holiday_dt,\n",
    "                'pre_period': (pre_holiday_start, pre_holiday_end),\n",
    "                'post_period': (post_holiday_start, post_holiday_end),\n",
    "                'peak_window': (peak_window_start, peak_window_end),\n",
    "                'early_start_lateend': (pre_holiday_earlystart, post_holiday_lateend)\n",
    "            }\n",
    "        return holiday_periods\n",
    "\n",
    "    def detect_peak_periods(self, time_series, dates_series, peak_window_indices):\n",
    "        \"\"\"\n",
    "        检测波峰是否在节日窗口期内\n",
    "        补充：如果全年搜索排名都为-1，也视为在窗口期内\n",
    "        \"\"\"\n",
    "        if len(time_series) == 0:\n",
    "            return False, None\n",
    "        \n",
    "        # 如果全年排名都为-1，直接返回True\n",
    "        if np.all(time_series == 100000):\n",
    "            return False, None\n",
    "        \n",
    "        # 找到最低排名（最高搜索量）的位置\n",
    "        min_rank_idx = np.argmin(time_series)\n",
    "        # 得到搜索量最高的日期 \n",
    "\n",
    "        # 检查是否在节日窗口期内, 并返回波峰的日期\n",
    "        return (peak_window_indices[0] <= min_rank_idx <= peak_window_indices[1]), dates_series[min_rank_idx]\n",
    "\n",
    "    def analyze_trend_pattern(self, time_series, pre_indices, post_indices):\n",
    "        \"\"\"\n",
    "        分析趋势模式：节前增长，节后下降\n",
    "        \"\"\"\n",
    "        if len(time_series) < 8:  # 需要足够的数据点\n",
    "            return False, 0, 0\n",
    "        \n",
    "        # 节前趋势分析\n",
    "        pre_data = time_series[pre_indices[0]:pre_indices[1]+1]\n",
    "        pre_x = np.arange(len(pre_data)).reshape(-1, 1)\n",
    "        \n",
    "        # 节后趋势分析\n",
    "        post_data = time_series[post_indices[0]:post_indices[1]+1]\n",
    "        post_x = np.arange(len(post_data)).reshape(-1, 1)\n",
    "        # 计算斜率\n",
    "        pre_slope = self.calculate_slope(pre_x, pre_data)\n",
    "        post_slope = self.calculate_slope(post_x, post_data)\n",
    "        \n",
    "        # 模式条件：节前下降（排名上升，搜索量增长），节后上升（排名下降，搜索量减少）\n",
    "        pattern_match = pre_slope <= 0 and post_slope >= 0 and (abs(pre_slope) <= abs(post_slope))\n",
    "        \n",
    "        return pattern_match, pre_slope, post_slope\n",
    "\n",
    "    def calculate_slope(self, x, y):\n",
    "        \"\"\"计算线性回归斜率\"\"\"\n",
    "        if len(y) < 2:\n",
    "            return 0\n",
    "        \n",
    "        try:\n",
    "            model = LinearRegression()\n",
    "            model.fit(x, y)\n",
    "            return model.coef_[0]\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def start_peak_end_dates(self, time_series, dates_series, earlystart_lateend_indices):\n",
    "        \"\"\"\n",
    "        挖掘关键的特征:\n",
    "        1. 启动期: 挖掘一个关键词在节日前第一次稳定持续出现在前10W排名的时期\n",
    "        2. 波峰期: 挖掘一个关键词在节日前搜索量最高的日期\n",
    "        3. 衰退期: 挖掘一个关键词在节日后搜索掉到10W+的日期\n",
    "        \"\"\"\n",
    "        start_idx, end_idx = earlystart_lateend_indices\n",
    "        ts = time_series[start_idx:end_idx+1]\n",
    "        ds = dates_series[start_idx:end_idx+1]\n",
    "        # 1) 启动期：第一次排名<10W\n",
    "        launch_date = None\n",
    "        for i in range(len(ts)):\n",
    "            if ts[i] < 100000:\n",
    "                launch_date = ds[i]\n",
    "                break\n",
    "        # 2) 波峰期：搜索量最高（排名最低）的日期\n",
    "        peak_date = ds[np.argmin(ts)] if len(ts) and np.min(ts) < 100000 else None\n",
    "        # 3) 衰退期：节日后第一次连续3天排名≥10W\n",
    "        decline_date = None\n",
    "        # 90+ 30天，取后75%作为“节后”范围\n",
    "        post_start = (len(ts) // 4) * 3 \n",
    "        for i in range(post_start, len(ts) - 1):\n",
    "            if all(ts[i+j] >= 100000 for j in range(2)):\n",
    "                decline_date = ds[i]\n",
    "                break\n",
    "\n",
    "        return launch_date, peak_date, decline_date\n",
    "        \n",
    "\n",
    "    def validate_multi_year_pattern(self, holiday_date, keyword_idx, holiday_periods):\n",
    "        \"\"\"\n",
    "        验证多年模式一致性\n",
    "        \"\"\"\n",
    "        year_results = {}\n",
    "        \n",
    "        for year, periods in holiday_periods.items():\n",
    "            # 获取该年份的时间段索引\n",
    "            holiday_dt = pd.to_datetime(f\"{year}-{holiday_date}\")\n",
    "            window_start = holiday_dt - timedelta(days=180)\n",
    "            window_end   = holiday_dt + timedelta(days=30)\n",
    "            year_mask = (self.dates >= window_start) & (self.dates <= window_end)\n",
    "            year_data = self.ts_data[keyword_idx][year_mask]\n",
    "            year_dates = self.dates[year_mask]\n",
    "            \n",
    "            if len(year_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            # 获取各时间段索引\n",
    "            pre_start_idx, pre_end_idx = self.get_period_indices(year_dates, periods['pre_period'])\n",
    "            post_start_idx, post_end_idx = self.get_period_indices(year_dates, periods['post_period'])\n",
    "            peak_start_idx, peak_end_idx = self.get_period_indices(year_dates, periods['peak_window'])\n",
    "            pre_holiday_earlystart, post_holiday_lateend = self.get_period_indices(year_dates, periods['early_start_lateend'])\n",
    "\n",
    "            if None in [pre_start_idx, pre_end_idx, post_start_idx, post_end_idx]:\n",
    "                continue\n",
    "                \n",
    "            # 检测波峰和模式\n",
    "            peak_in_window, peak_date = self.detect_peak_periods(year_data, year_dates, (peak_start_idx, peak_end_idx))\n",
    "            \n",
    "            pattern_match, pre_slope, post_slope = self.analyze_trend_pattern(\n",
    "                year_data, (pre_start_idx, pre_end_idx), (post_start_idx, post_end_idx)\n",
    "            )\n",
    "            # 计算启动日期、波峰日期、衰退日期\n",
    "            launch_date, true_peak_date, decline_date = self.start_peak_end_dates(\n",
    "                year_data, year_dates, (pre_holiday_earlystart, post_holiday_lateend)\n",
    "            )\n",
    "            \n",
    "            year_results[year] = {\n",
    "                'launch_date': launch_date,\n",
    "                'true_peak_date': true_peak_date,\n",
    "                'decline_date': decline_date,\n",
    "                'min_rank': np.min(year_data) if len(year_data) > 0 else 100000,\n",
    "                'pre_slope': pre_slope,\n",
    "                'post_slope': post_slope,   \n",
    "                'peak_date': peak_date,\n",
    "                'peak_in_window': peak_in_window,\n",
    "                'pattern_match': pattern_match\n",
    "            }\n",
    "        \n",
    "        return year_results\n",
    "\n",
    "    def get_period_indices(self, year_dates, period):\n",
    "        \"\"\"获取时间段对应的索引\"\"\"\n",
    "        start_idx = np.where(year_dates >= period[0])[0]\n",
    "        end_idx = np.where(year_dates <= period[1])[0]\n",
    "        \n",
    "        if len(start_idx) == 0 or len(end_idx) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        return start_idx[0], end_idx[-1]\n",
    "\n",
    "\n",
    "    def mine_holiday_keywords(self, holiday_date, years=[2023, 2024, 2025], \n",
    "                         min_consistency=0.8, white_word=[]):\n",
    "        \"\"\"\n",
    "        主函数：挖掘节日相关关键词\n",
    "        \"\"\"\n",
    "        # 定义节日期间\n",
    "        holiday_periods = self.define_holiday_periods(holiday_date, years)\n",
    "\n",
    "        results = []\n",
    "        for idx, keyword in enumerate(self.keywords):\n",
    "            if idx % 1000 == 0:\n",
    "                print(f\"处理进度: {idx}/{len(self.keywords)}\")\n",
    "            # 验证多年模式\n",
    "            year_results = self.validate_multi_year_pattern(holiday_date, idx, holiday_periods)\n",
    "            if len(year_results) < len(years) * min_consistency:\n",
    "                continue\n",
    "            # 计算模式一致性\n",
    "            peak_consistency = sum([r['peak_in_window'] for r in year_results.values()]) / len(year_results)\n",
    "            pattern_consistency = sum([r['pattern_match'] for r in year_results.values()]) / len(year_results)\n",
    "            \n",
    "            hit_wight_words = False \n",
    "            for word in white_word:\n",
    "                if word in keyword:\n",
    "                    hit_wight_words = True\n",
    "                    break\n",
    "            # 过滤条件\n",
    "            if (peak_consistency >= min_consistency and pattern_consistency >= min_consistency) or hit_wight_words:\n",
    "                # 计算节前4周平均斜率\n",
    "                avg_pre_slope = np.mean([r['pre_slope'] for r in year_results.values()])\n",
    "                # 计算节后4周平均斜率\n",
    "                avg_post_slope = np.mean([r['post_slope'] for r in year_results.values()])\n",
    "                # 最高ABA排名（最低排名值）\n",
    "                best_rank = min([r['min_rank'] for r in year_results.values()])\n",
    "                # 最近一年的ABA相比上一年的ABA排名增加情况 \n",
    "                rank_increase = year_results[years[-2]]['min_rank'] - year_results[years[-1]]['min_rank']\n",
    "                rank_increase_ratio = rank_increase / year_results[years[-2]]['min_rank']\n",
    "                # 最近一年的节前搜索增长斜率 - 上一年搜索增长斜率情况 \n",
    "                slope_increase = abs(year_results[years[-1]]['pre_slope'] - abs(year_results[years[-2]]['pre_slope']))\n",
    "\n",
    "                last_year_result = year_results.get(years[-1], None)\n",
    "                results.append({\n",
    "                    'keyword': keyword,\n",
    "                    # 去年的启动日期、波峰日期、衰退日期\n",
    "                    'last_year_launch_date':  last_year_result['launch_date'] if last_year_result else None,\n",
    "                    'last_year_true_peak_date':  last_year_result['true_peak_date'] if last_year_result else None,\n",
    "                    'last_year_decline_date':  last_year_result['decline_date'] if last_year_result else None,\n",
    "                    # 最小的ABA排名（最低排名值）\n",
    "                    'last_year_min_rank':  last_year_result['min_rank'] if last_year_result else None,\n",
    "                    # 去年的节前搜索增长斜率\n",
    "                    'last_year_pre_slope':  last_year_result['pre_slope'] if last_year_result else None,\n",
    "                    # 去年的节后搜索增长斜率\n",
    "                    'last_year_post_slope':  last_year_result['post_slope'] if last_year_result else None,\n",
    "                    # 每年的对比\n",
    "                    'best_aba_rank': best_rank, # 平均每年的最佳ABA排名\n",
    "                    'avg_pre_slope': avg_pre_slope, # 平均每年的节前4周平均斜率\n",
    "                    'avg_post_slope': avg_post_slope, # 平均每年的节后4周平均斜率\n",
    "                    'slope_increase': slope_increase, # 最近一年节前斜率相对上一年的增幅\n",
    "                    'rank_increase': rank_increase, # 最近一年ABA排名相对上一年的增幅\n",
    "                    'rank_increase_ratio': rank_increase_ratio, # 最近一年ABA排名相对上一年的增幅\n",
    "                    # 每年的原始数据\n",
    "                    'year_results': year_results\n",
    "                })\n",
    "       \n",
    "        results.sort(key=lambda x: x['avg_post_slope'], reverse=True)\n",
    "        \n",
    "        return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c77c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# 假设 volcenginesdkarkruntime 已安装，Ark 可用\n",
    "try:\n",
    "    from volcenginesdkarkruntime import Ark\n",
    "except ImportError:\n",
    "    Ark = None\n",
    "\n",
    "class KeyWordFilterAnalysis(BaseModel):\n",
    "    # 根据实际返回结构定义，这里仅示例\n",
    "    non_holiday_keywords: List[str]\n",
    "\n",
    "def call_ark(user_msg: List[Dict], prompt: str, api_key: str, model: str) -> KeyWordFilterAnalysis:\n",
    "    if Ark is None:\n",
    "        raise RuntimeError(\"未安装 volcenginesdkarkruntime，请先在 requirements.txt 中添加并安装\")\n",
    "    client = Ark(api_key=api_key)\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": user_msg}],\n",
    "        response_format=KeyWordFilterAnalysis,\n",
    "        extra_body={\"thinking\": {\"type\": \"disabled\"}},\n",
    "    )\n",
    "    return completion.choices[0].message.parsed\n",
    "\n",
    "class HolidayKeywordFilter:\n",
    "    def __init__(self, api_key: str = '29549de0-26ea-4e17-b73f-09ecdf08b678', model: str = \"ep-20250618020820-t2x6m\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "\n",
    "    def filter_keywords_by_llm(self, keyword_list: list, holiday_date: str) -> list:\n",
    "        \"\"\"\n",
    "        给定一组关键词，是我根据在节日前搜索增加节日后搜索降低的趋势挖掘的可能属于和该节日相关的关键词。\n",
    "        现在需要你根据LLM去掉和该节日不相关的关键词\n",
    "\n",
    "        使用大语言模型，每50个词一组，判断是否符合节日特征。\n",
    "        使用大语言模型返回其中不符合的关键词，组成一个新的列表。\n",
    "        \n",
    "        Args:\n",
    "            keyword_list: 输入的关键词列表\n",
    "            holiday_date: 节日日期\n",
    "            \n",
    "        Returns:\n",
    "            list: 符合节日特征的关键词列表\n",
    "        \"\"\"\n",
    "        # 每50个关键词一组，分批调用大模型\n",
    "        batch_size = 50\n",
    "        filtered_keywords = []\n",
    "        total_batches = (len(keyword_list) + batch_size - 1) // batch_size\n",
    "\n",
    "        for i in range(0, len(keyword_list), batch_size):\n",
    "            batch = keyword_list[i:i + batch_size]\n",
    "            current_batch = i // batch_size + 1\n",
    "            print(f\"[进度] 正在处理第 {current_batch}/{total_batches} 批关键词（共 {len(batch)} 个）...\")\n",
    "            \n",
    "            user_msg = {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": (\n",
    "                    f\"节日：{holiday_date}\\n\"\n",
    "                    \"给定一组关键词，是我根据在亚马逊的ABA搜索词排名数据。符合节日前搜索增加，节日后搜索降低的趋势，挖掘的可能属于和该节日相关的关键词。\\n\"\n",
    "                    \"但是里面可能有不相关的词。现在需要你根据LLM去掉和该节日不相关的关键词。\\n\"\n",
    "                    \"请返回一个列表，仅包含明显与节日不相关的关键词（即需要剔除的词）。\\n\"\n",
    "                    \"如果全部相关，返回空列表[]。\\n\"\n",
    "                    \"关键词列表：\\n\" + \"\\n\".join(batch)\n",
    "                )\n",
    "            }\n",
    "            try:\n",
    "                # 调用封装后的 Ark 接口\n",
    "                result = call_ark(\n",
    "                    user_msg=[user_msg],\n",
    "                    prompt='你是一个专业的关键词筛选器，负责根据给定的节日日期和关键词列表，判断哪些关键词与该节日相关，哪些不相关。',\n",
    "                    api_key=self.api_key,\n",
    "                    model=self.model\n",
    "                )\n",
    "                non_holiday_keywords = result.non_holiday_keywords\n",
    "                # 从当前批次中剔除这些词\n",
    "                filtered_batch = [kw for kw in batch if kw not in non_holiday_keywords]\n",
    "                filtered_keywords.extend(filtered_batch)\n",
    "                print(f\"[进度] 第 {current_batch} 批完成，剔除 {len(non_holiday_keywords)} 个关键词，保留 {len(filtered_batch)} 个。\")\n",
    "            except Exception as e:\n",
    "                # 解析异常时默认保留全部\n",
    "                filtered_keywords.extend(batch)\n",
    "                print(f\"[进度] 第 {current_batch} 批解析异常（{e}），默认保留全部 {len(batch)} 个关键词。\")\n",
    "\n",
    "        print(f\"[完成] 全部批次处理完毕，最终保留 {len(filtered_keywords)} 个关键词。\")\n",
    "        return filtered_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83a6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理进度: 0/132913\n",
      "处理进度: 1000/132913\n",
      "处理进度: 2000/132913\n",
      "处理进度: 3000/132913\n",
      "处理进度: 4000/132913\n",
      "处理进度: 5000/132913\n",
      "处理进度: 6000/132913\n",
      "处理进度: 7000/132913\n",
      "处理进度: 8000/132913\n",
      "处理进度: 9000/132913\n",
      "处理进度: 10000/132913\n",
      "处理进度: 11000/132913\n",
      "处理进度: 12000/132913\n",
      "处理进度: 13000/132913\n",
      "处理进度: 14000/132913\n",
      "处理进度: 15000/132913\n",
      "处理进度: 16000/132913\n",
      "处理进度: 17000/132913\n",
      "处理进度: 18000/132913\n",
      "处理进度: 19000/132913\n",
      "处理进度: 20000/132913\n",
      "处理进度: 21000/132913\n",
      "处理进度: 22000/132913\n",
      "处理进度: 23000/132913\n",
      "处理进度: 24000/132913\n",
      "处理进度: 25000/132913\n",
      "处理进度: 26000/132913\n",
      "处理进度: 27000/132913\n",
      "处理进度: 28000/132913\n",
      "处理进度: 29000/132913\n",
      "处理进度: 30000/132913\n",
      "处理进度: 31000/132913\n",
      "处理进度: 32000/132913\n",
      "处理进度: 33000/132913\n",
      "处理进度: 34000/132913\n",
      "处理进度: 35000/132913\n",
      "处理进度: 36000/132913\n",
      "处理进度: 37000/132913\n",
      "处理进度: 38000/132913\n",
      "处理进度: 39000/132913\n",
      "处理进度: 40000/132913\n",
      "处理进度: 41000/132913\n",
      "处理进度: 42000/132913\n",
      "处理进度: 43000/132913\n",
      "处理进度: 44000/132913\n",
      "处理进度: 45000/132913\n",
      "处理进度: 46000/132913\n",
      "处理进度: 47000/132913\n",
      "处理进度: 48000/132913\n",
      "处理进度: 49000/132913\n",
      "处理进度: 50000/132913\n",
      "处理进度: 51000/132913\n",
      "处理进度: 52000/132913\n",
      "处理进度: 53000/132913\n",
      "处理进度: 54000/132913\n",
      "处理进度: 55000/132913\n",
      "处理进度: 56000/132913\n",
      "处理进度: 57000/132913\n",
      "处理进度: 58000/132913\n",
      "处理进度: 59000/132913\n",
      "处理进度: 60000/132913\n",
      "处理进度: 61000/132913\n",
      "处理进度: 62000/132913\n",
      "处理进度: 63000/132913\n",
      "处理进度: 64000/132913\n",
      "处理进度: 65000/132913\n",
      "处理进度: 66000/132913\n",
      "处理进度: 67000/132913\n",
      "处理进度: 68000/132913\n",
      "处理进度: 69000/132913\n",
      "处理进度: 70000/132913\n",
      "处理进度: 71000/132913\n",
      "处理进度: 72000/132913\n",
      "处理进度: 73000/132913\n",
      "处理进度: 74000/132913\n",
      "处理进度: 75000/132913\n",
      "处理进度: 76000/132913\n",
      "处理进度: 77000/132913\n",
      "处理进度: 78000/132913\n",
      "处理进度: 79000/132913\n",
      "处理进度: 80000/132913\n",
      "处理进度: 81000/132913\n",
      "处理进度: 82000/132913\n",
      "处理进度: 83000/132913\n",
      "处理进度: 84000/132913\n",
      "处理进度: 85000/132913\n",
      "处理进度: 86000/132913\n",
      "处理进度: 87000/132913\n",
      "处理进度: 88000/132913\n",
      "处理进度: 89000/132913\n",
      "处理进度: 90000/132913\n",
      "处理进度: 91000/132913\n",
      "处理进度: 92000/132913\n",
      "处理进度: 93000/132913\n",
      "处理进度: 94000/132913\n",
      "处理进度: 95000/132913\n",
      "处理进度: 96000/132913\n",
      "处理进度: 97000/132913\n",
      "处理进度: 98000/132913\n",
      "处理进度: 99000/132913\n",
      "处理进度: 100000/132913\n",
      "处理进度: 101000/132913\n",
      "处理进度: 102000/132913\n",
      "处理进度: 103000/132913\n",
      "处理进度: 104000/132913\n",
      "处理进度: 105000/132913\n",
      "处理进度: 106000/132913\n",
      "处理进度: 107000/132913\n",
      "处理进度: 108000/132913\n",
      "处理进度: 109000/132913\n",
      "处理进度: 110000/132913\n",
      "处理进度: 111000/132913\n",
      "处理进度: 112000/132913\n",
      "处理进度: 113000/132913\n",
      "处理进度: 114000/132913\n",
      "处理进度: 115000/132913\n",
      "处理进度: 116000/132913\n",
      "处理进度: 117000/132913\n",
      "处理进度: 118000/132913\n",
      "处理进度: 119000/132913\n",
      "处理进度: 120000/132913\n",
      "处理进度: 121000/132913\n",
      "处理进度: 122000/132913\n",
      "处理进度: 123000/132913\n",
      "处理进度: 124000/132913\n",
      "处理进度: 125000/132913\n",
      "处理进度: 126000/132913\n",
      "处理进度: 127000/132913\n",
      "处理进度: 128000/132913\n",
      "处理进度: 129000/132913\n",
      "处理进度: 130000/132913\n",
      "处理进度: 131000/132913\n",
      "处理进度: 132000/132913\n",
      "[进度] 正在处理第 1/109 批关键词（共 50 个）...\n",
      "[进度] 第 1 批完成，剔除 4 个关键词，保留 46 个。\n",
      "[进度] 正在处理第 2/109 批关键词（共 50 个）...\n",
      "[进度] 第 2 批完成，剔除 3 个关键词，保留 47 个。\n",
      "[进度] 正在处理第 3/109 批关键词（共 50 个）...\n",
      "[进度] 第 3 批完成，剔除 4 个关键词，保留 46 个。\n",
      "[进度] 正在处理第 4/109 批关键词（共 50 个）...\n",
      "[进度] 第 4 批完成，剔除 22 个关键词，保留 28 个。\n",
      "[进度] 正在处理第 5/109 批关键词（共 50 个）...\n",
      "[进度] 第 5 批完成，剔除 12 个关键词，保留 38 个。\n",
      "[进度] 正在处理第 6/109 批关键词（共 50 个）...\n",
      "[进度] 第 6 批完成，剔除 4 个关键词，保留 46 个。\n",
      "[进度] 正在处理第 7/109 批关键词（共 50 个）...\n",
      "[进度] 第 7 批完成，剔除 8 个关键词，保留 42 个。\n",
      "[进度] 正在处理第 8/109 批关键词（共 50 个）...\n",
      "[进度] 第 8 批完成，剔除 21 个关键词，保留 29 个。\n",
      "[进度] 正在处理第 9/109 批关键词（共 50 个）...\n",
      "[进度] 第 9 批完成，剔除 15 个关键词，保留 35 个。\n",
      "[进度] 正在处理第 10/109 批关键词（共 50 个）...\n",
      "[进度] 第 10 批完成，剔除 6 个关键词，保留 44 个。\n",
      "[进度] 正在处理第 11/109 批关键词（共 50 个）...\n",
      "[进度] 第 11 批完成，剔除 10 个关键词，保留 40 个。\n",
      "[进度] 正在处理第 12/109 批关键词（共 50 个）...\n",
      "[进度] 第 12 批完成，剔除 14 个关键词，保留 36 个。\n",
      "[进度] 正在处理第 13/109 批关键词（共 50 个）...\n",
      "[进度] 第 13 批完成，剔除 11 个关键词，保留 39 个。\n",
      "[进度] 正在处理第 14/109 批关键词（共 50 个）...\n",
      "[进度] 第 14 批完成，剔除 15 个关键词，保留 35 个。\n",
      "[进度] 正在处理第 15/109 批关键词（共 50 个）...\n",
      "[进度] 第 15 批完成，剔除 13 个关键词，保留 37 个。\n",
      "[进度] 正在处理第 16/109 批关键词（共 50 个）...\n",
      "[进度] 第 16 批完成，剔除 31 个关键词，保留 19 个。\n",
      "[进度] 正在处理第 17/109 批关键词（共 50 个）...\n",
      "[进度] 第 17 批完成，剔除 15 个关键词，保留 35 个。\n",
      "[进度] 正在处理第 18/109 批关键词（共 50 个）...\n",
      "[进度] 第 18 批完成，剔除 5 个关键词，保留 45 个。\n",
      "[进度] 正在处理第 19/109 批关键词（共 50 个）...\n",
      "[进度] 第 19 批完成，剔除 5 个关键词，保留 45 个。\n",
      "[进度] 正在处理第 20/109 批关键词（共 50 个）...\n",
      "[进度] 第 20 批完成，剔除 14 个关键词，保留 36 个。\n",
      "[进度] 正在处理第 21/109 批关键词（共 50 个）...\n",
      "[进度] 第 21 批完成，剔除 9 个关键词，保留 41 个。\n",
      "[进度] 正在处理第 22/109 批关键词（共 50 个）...\n",
      "[进度] 第 22 批完成，剔除 5 个关键词，保留 45 个。\n",
      "[进度] 正在处理第 23/109 批关键词（共 50 个）...\n",
      "[进度] 第 23 批完成，剔除 10 个关键词，保留 40 个。\n",
      "[进度] 正在处理第 24/109 批关键词（共 50 个）...\n",
      "[进度] 第 24 批完成，剔除 8 个关键词，保留 42 个。\n",
      "[进度] 正在处理第 25/109 批关键词（共 50 个）...\n",
      "[进度] 第 25 批完成，剔除 7 个关键词，保留 43 个。\n",
      "[进度] 正在处理第 26/109 批关键词（共 50 个）...\n",
      "[进度] 第 26 批完成，剔除 4 个关键词，保留 46 个。\n",
      "[进度] 正在处理第 27/109 批关键词（共 50 个）...\n",
      "[进度] 第 27 批完成，剔除 6 个关键词，保留 44 个。\n",
      "[进度] 正在处理第 28/109 批关键词（共 50 个）...\n",
      "[进度] 第 28 批完成，剔除 7 个关键词，保留 43 个。\n",
      "[进度] 正在处理第 29/109 批关键词（共 50 个）...\n",
      "[进度] 第 29 批完成，剔除 3 个关键词，保留 47 个。\n",
      "[进度] 正在处理第 30/109 批关键词（共 50 个）...\n",
      "[进度] 第 30 批完成，剔除 5 个关键词，保留 45 个。\n",
      "[进度] 正在处理第 31/109 批关键词（共 50 个）...\n",
      "[进度] 第 31 批完成，剔除 6 个关键词，保留 44 个。\n",
      "[进度] 正在处理第 32/109 批关键词（共 50 个）...\n",
      "[进度] 第 32 批完成，剔除 31 个关键词，保留 19 个。\n",
      "[进度] 正在处理第 33/109 批关键词（共 50 个）...\n",
      "[进度] 第 33 批完成，剔除 9 个关键词，保留 41 个。\n",
      "[进度] 正在处理第 34/109 批关键词（共 50 个）...\n",
      "[进度] 第 34 批完成，剔除 5 个关键词，保留 45 个。\n",
      "[进度] 正在处理第 35/109 批关键词（共 50 个）...\n",
      "[进度] 第 35 批完成，剔除 7 个关键词，保留 43 个。\n",
      "[进度] 正在处理第 36/109 批关键词（共 50 个）...\n",
      "[进度] 第 36 批完成，剔除 11 个关键词，保留 39 个。\n",
      "[进度] 正在处理第 37/109 批关键词（共 50 个）...\n",
      "[进度] 第 37 批完成，剔除 16 个关键词，保留 34 个。\n",
      "[进度] 正在处理第 38/109 批关键词（共 50 个）...\n"
     ]
    }
   ],
   "source": [
    "aba_file_path = 'data/df_10w_aba.csv' \n",
    "\n",
    "holiday = [\n",
    "    # {\n",
    "        \n",
    "    #     'date': '01-01',\n",
    "    #     'name': 'new_year',\n",
    "    #     'white_word': [\"new year\", \"happy new year\", \"resolution\", \"new year's eve\", \"countdown\"]\n",
    "    # },\n",
    "    # {\n",
    "    #     'date': '02-14',\n",
    "    #     'name': 'valentine',\n",
    "    #     'white_word': [\"valentine\", \"valentine's\", \"love\"]\n",
    "    # }\n",
    "    # {\n",
    "    #     'date': '03-17',\n",
    "    #     'name': 'st_patricks_day',\n",
    "    #     'white_word': [\"st patrick\", \"st. patrick\", \"irish\"]\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': 'easter',\n",
    "    #     'date': '04-20', # 仅作为示例参考，实际需要计算\n",
    "    #     'white_word': [\"easter\", \"bunny\", \"egg hunt\", \"easter basket\", \"spring\"]\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': 'mothers_day',\n",
    "    #     'date': '05-11', \n",
    "    #     'white_word': [\"mother's day\", \"mom\", \"gifts for mom\", \"best mom\", \"mama\"]\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': 'memorial_day',\n",
    "    #     'date': '05-25', # 仅作为示例参考，实际需要计算\n",
    "    #     'white_word': [\"memorial day\", \"bbq\", \"outdoor\", \"summer start\", \"flag\", \"party\"]\n",
    "    # },\n",
    "    # {\n",
    "    #     'date': '06-01',\n",
    "    #     'name': 'childrens_day',\n",
    "    #     'white_word': [\"children's day\", \"kids\", \"toys\"]\n",
    "    # },\n",
    "    # {\n",
    "    #     'date': '06-21',  # 使用特殊标识符\n",
    "    #     'name': 'fathers_day',\n",
    "    #     'white_word': [\"father's day\", \"dad\", \"father\"]\n",
    "    # },\n",
    "    # {\n",
    "    #     'date': '07-04',\n",
    "    #     'name': 'independence_day',\n",
    "    #     'white_word': [\"independence day\", \"4th of july\", \"fireworks\", 'patriotic', 'independence']\n",
    "    # },\n",
    "    {\n",
    "        'date': '10-31',\n",
    "        'name': 'halloween',\n",
    "        'white_word': [\"halloween\", \"costume\", \"candy\"]\n",
    "    },\n",
    "    {\n",
    "        'date': '11-27',  # 使用特殊标识符\n",
    "        'name': 'thanksgiving',\n",
    "        'white_word': [\"thanksgiving\", \"turkey\", \"dinner\"]\n",
    "    },\n",
    "    {\n",
    "        'date': '12-25',\n",
    "        'name': 'christmas',\n",
    "        'white_word': [\"christmas\", \"xmas\", \"gift\"]\n",
    "    }\n",
    "]\n",
    "miner = HolidayKeywordMiner(aba_file_path)\n",
    "\n",
    "for h in holiday:\n",
    "    holiday_keyword_step1_df = miner.mine_holiday_keywords(\n",
    "        holiday_date=h['date'],  #对应节日\n",
    "        years=[2023,2024],\n",
    "        min_consistency=0.8, # 80%的一致性要求\n",
    "        white_word=h['white_word']\n",
    "    )\n",
    "    valid_keywords = list(holiday_keyword_step1_df.keyword.values)\n",
    "    hkf = HolidayKeywordFilter()\n",
    "    filter_result = hkf.filter_keywords_by_llm(keyword_list=valid_keywords, holiday_date=h['name'])\n",
    "    # 只保留 filter_result 中的keyword列的值，存在的 keywords数组中的行\n",
    "    holiday_keyword_df = holiday_keyword_step1_df[holiday_keyword_step1_df['keyword'].isin(filter_result)]\n",
    "    holiday_keyword_df.to_csv(f'result/{h[\"name\"]}_keyword_info.csv', index=False)\n",
    "    \n",
    "    # 得到原始的日期数据\n",
    "    dates_data = pd.read_csv(aba_file_path)\n",
    "    holiday_info_data = pd.read_csv(f'result/{h[\"name\"]}_keyword_info.csv')\n",
    "\n",
    "    holiday_keywords = holiday_info_data.keyword.values\n",
    "\n",
    "    dates_data[dates_data.keyword.isin(holiday_keywords)].to_csv(f'result/{h[\"name\"]}_keyword_trends.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa8b6673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holiday_keyword_step1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa11d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
