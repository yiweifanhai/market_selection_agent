{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "655da7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ea45161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/Users/shuxin/projects/market_analysis/LLM_market_selection/V2/data/20250802_all_10w_with_rank_ratio.csv')\n",
    "# df.rename(columns={df.columns[0]: 'keyword'}, inplace=True)\n",
    "# new_df = df[(df['price_avg'] >= 20) & (df['m1_mid'] >= 0.50)]\n",
    "# print(f\"主数据筛选后数量: {len(new_df)}\")\n",
    "\n",
    "# df2 = pd.read_csv('/Users/shuxin/projects/market_analysis/LLM_market_selection/NR/new_release_20250830.csv')\n",
    "# new_df2 = df2[(df2['price_avg'] >= 15) & (df2['m1_mid'] >= 0.45)]\n",
    "\n",
    "# merged_df = pd.concat([\n",
    "#     new_df[~new_df['keyword'].isin(new_df2['keyword'])],\n",
    "#     new_df2\n",
    "# ], ignore_index=True)\n",
    "# print(f\"合并去重后总数量: {len(merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "714d564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3348\n"
     ]
    }
   ],
   "source": [
    "# 按照要求筛选数据：price_avg >= 18 且 m1_mid > 0.55\n",
    "# new_df = df[((df['price_avg'] >= 20) & (df['m1_mid'] >= 0.50)) | (df['relative_available_date_days_mid'] < 365)]\n",
    "import os \n",
    "import time \n",
    "\n",
    "# 在jupyter notebook中获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "folder_path = '/Users/shuxin/projects/market_analysis/LLM_market_selection/V2/'\n",
    "file_path = 'data/20250913_part_1_with_rank_ratio.csv'\n",
    "# 定义中间结果保存文件名\n",
    "\n",
    "file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "temp_output_file = current_dir + f'/data/{file_name}_material_analysis_results_temp_{int(time.time())}.csv'\n",
    "output_file = current_dir + '/data/' + file_name + '_material_analysis_results.csv'\n",
    "image_save_dir = current_dir + f\"/images/downloaded_images_{file_name}\"\n",
    "\n",
    "def get_df():\n",
    "    df= all_word = pd.read_csv(folder_path + file_path)\n",
    "    df = df.rename(columns={all_word.columns[0]: 'keyword'})\n",
    "    new_df = df[((df['price_avg'] >= 20) \n",
    "              & (df['m1_mid'] >= 0.50) \n",
    "              # & (df['cr_by_product_cr5'] <= 0.8) \n",
    "                # & (df['score'] >= 6.5) \n",
    "              )]\n",
    "    print(len(new_df))\n",
    "    return new_df\n",
    "\n",
    "merged_df = get_df() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc8ec8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import imghdr\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    将图片编码为base64格式，并返回data URI格式的字符串\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_data = image_file.read()\n",
    "        # 检测图片类型\n",
    "        img_type = imghdr.what(None, image_data)\n",
    "        if img_type is None:\n",
    "            img_type = image_path.split('.')[-1]\n",
    "        # 转换为base64并拼接data URI\n",
    "        base64_image = base64.b64encode(image_data).decode('utf-8')\n",
    "        return f\"data:image/{img_type};base64,{base64_image}\"\n",
    "\n",
    "def find_images_by_keyword(keyword, download_images_dir=image_save_dir):\n",
    "    \"\"\"\n",
    "    根据关键词查找以 keyword_ 开头的图片\n",
    "    返回图片的base64编码\n",
    "    \"\"\"\n",
    "    base64_images = []\n",
    "    # 确保目录路径存在\n",
    "    if not os.path.exists(download_images_dir):\n",
    "        return []\n",
    "    prefix = f\"{keyword}_\"\n",
    "    # 遍历目录下的所有文件\n",
    "    for filename in os.listdir(download_images_dir):\n",
    "        # 检查文件是否以关键词_开头，且是图片文件\n",
    "        if filename.startswith(prefix) and any(filename.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "            image_file_path = os.path.join(download_images_dir, filename)\n",
    "            base64_images.append(encode_image(image_file_path))\n",
    "            \n",
    "    return base64_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7a59d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "# 测试调用方舟模型，给定schema，输出json \n",
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "# 初始化方舟SDK客户端\n",
    "client = Ark(\n",
    "    # 从环境变量获取方舟API Key（需提前设置环境变量）\n",
    "    api_key=\"29549de0-26ea-4e17-b73f-09ecdf08b678\",\n",
    ")\n",
    "\n",
    "# 产品材质分析模型\n",
    "class ProductMaterialAnalysis(BaseModel):\n",
    "    easy_to_develop_reason: str  # 容易开发的理由\n",
    "    easy_to_develop_conclusion: str  # 是否容易开发该产品，不需要开模就可以低成本快速改造的\n",
    "    material_reason: str  # 理由\n",
    "    material_conclusion: str   # 最终的判断结果\n",
    "\n",
    "prompt = \"\"\"\n",
    "    你是一位专业的产品材质分析师，现在需要你根据一个关键词对应的市场下的几个产品的图片，以及产品的标题判断材质以及改造生产难度\n",
    "\n",
    "    # 输入数据\n",
    "        1. 关键词\n",
    "        2. 前5个产品的标题\n",
    "        3. 关键词对应的市场的一级分类\n",
    "        4. 二级分类\n",
    "        5. 三级分类\n",
    "        6. 5个产品的图片\n",
    "\n",
    "    # 主要工作\n",
    "    1. 判断该市场的产品的材质。材质需要为金属、陶瓷、玻璃、塑料、橡胶、布料纤维、纸张、骨头、木材、皮革、硅胶、石头珠宝、液体、碳/石墨材料中的一种，如果不属于其中的材质，请输出你认为的材质，如果是电子电器产品不需要判断材质，结果判断为电子产品。\n",
    "    2. 判断产品是否容易进行差异化改造的，从生产的角度判断是否容易进行低成本的差异化改造和生产。如3C电子电器产品需要开模会比较贵，但是陶瓷、玻璃、木材、纸张等材质的产品容易进行低成本的差异化改造。\n",
    "    3. 给出最终的判断结果。\n",
    "    \n",
    "    # 输出\n",
    "    easy_to_develop_reason # 是否容易开发该产品，不需要复杂的生产流程就可以低成本快速改造的理由\n",
    "    easy_to_develop_conclusion # 是否容易开发该产品的结论\n",
    "    material_reason # 材质判断的理由\n",
    "    material_conclusion # 最终的材质判断结果\n",
    "\n",
    "    所有输出必须为中文。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_response(item, schema=ProductMaterialAnalysis):\n",
    "    keyword = item['keyword']\n",
    "    images = find_images_by_keyword(keyword)\n",
    "    top5_title = item['top5_title'].split(';;')\n",
    "    common_category_l1 = item['common_category_l1']\n",
    "    common_category_l2 = item['common_category_l2']\n",
    "    common_category_l3 = item['common_category_l3']\n",
    "\n",
    "    user_msg = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"关键词：{keyword}\\n前5个产品的标题分别为：{top5_title}\\n关键词对应的市场的一级分类：{common_category_l1}\\n二级分类：{common_category_l2}\\n三级分类：{common_category_l3} 其中5个产品的图片分别为：\"\n",
    "        }\n",
    "    ]\n",
    "    for image in images:\n",
    "        user_msg.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\":  image\n",
    "            },         \n",
    "        })\n",
    "    user_msg.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"请根据以上图片和文字，判断该市场的产品的材质以及理由。先给出理由，才给出最终的材质结果。\"\n",
    "    })\n",
    "    \n",
    "    # 调用方舟模型生成响应（自动解析为指定模型），并强制要求返回中文\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"doubao-seed-1-6-250615\",  # 具体模型需替换为实际可用模型\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ],\n",
    "        response_format=schema,  # 指定响应解析模型\n",
    "        extra_body={\n",
    "            \"thinking\": {\n",
    "                \"type\": \"disabled\" # 不使用深度思考能力\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    # 提取解析后的结构化响应\n",
    "    resp = completion.choices[0].message.parsed\n",
    "    return resp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "# Define kept columns\n",
    "kept_cols = ['keyword', 'top5_images', 'top5_title', 'common_category_l1', 'common_category_l2', 'common_category_l3']\n",
    "\n",
    "# Prepare result DataFrame\n",
    "result_df = merged_df[kept_cols].copy()\n",
    "result_df['easy_to_develop_reason'] = ''\n",
    "result_df['easy_to_develop_conclusion'] = ''\n",
    "result_df['material_reason'] = ''\n",
    "result_df['material_conclusion'] = ''\n",
    "\n",
    "\n",
    "\n",
    "def process_row(args):\n",
    "    index, row = args\n",
    "    try:\n",
    "        item = row.to_dict()\n",
    "        resp = get_response(item)  # Assumes get_response is defined\n",
    "        return {\n",
    "            'index': index,\n",
    "            'easy_to_develop_reason': resp.easy_to_develop_reason,\n",
    "            'easy_to_develop_conclusion': resp.easy_to_develop_conclusion,\n",
    "            'material_reason': resp.material_reason,\n",
    "            'material_conclusion': resp.material_conclusion\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "        return {\n",
    "            'index': index,\n",
    "            'easy_to_develop_reason': 'Error',\n",
    "            'easy_to_develop_conclusion': 'Error',\n",
    "            'material_reason': 'Error',\n",
    "            'material_conclusion': 'Error'\n",
    "        }\n",
    "\n",
    "# Process rows using ThreadPoolExecutor\n",
    "max_workers = 4  # Adjust based on system and workload\n",
    "results = []\n",
    "processed_count = 0\n",
    "save_interval = 10  # 每处理10条数据保存一次\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = []\n",
    "    for args in merged_df.iterrows():\n",
    "        future = executor.submit(process_row, args)\n",
    "        futures.append(future)\n",
    "    \n",
    "    # 使用tqdm显示进度\n",
    "    for future in tqdm(futures, total=len(merged_df), desc=\"Processing rows\"):\n",
    "        result = future.result()\n",
    "        results.append(result)\n",
    "        processed_count += 1\n",
    "        \n",
    "        # 更新DataFrame并定期保存\n",
    "        index = result['index']\n",
    "        result_df.loc[index, 'easy_to_develop_reason'] = result['easy_to_develop_reason']\n",
    "        result_df.loc[index, 'easy_to_develop_conclusion'] = result['easy_to_develop_conclusion']\n",
    "        result_df.loc[index, 'material_reason'] = result['material_reason']\n",
    "        result_df.loc[index, 'material_conclusion'] = result['material_conclusion']\n",
    "        \n",
    "        if processed_count % save_interval == 0:\n",
    "            # 保存中间结果\n",
    "            result_df.to_csv(temp_output_file, index=False)\n",
    "            print(f\"Saved intermediate results after processing {processed_count} items\")\n",
    "\n",
    "# Save final results to CSV\n",
    "\n",
    "result_df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# 删除临时文件\n",
    "if os.path.exists(temp_output_file):\n",
    "    os.remove(temp_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39ad7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "material_analysis_result = pd.read_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e57b55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('/Users/shuxin/projects/market_analysis/LLM_market_selection/V2/data/20250913_part_1-merged_features.csv')\n",
    "\n",
    "# 将material_analysis_result和x按keyword列进行合并\n",
    "merged_df = pd.merge(material_analysis_result, x, on='keyword', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "83541343",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\n",
    "    'keyword',\n",
    "    'top5_images',\n",
    "    'score',\n",
    "    'conclusion',\n",
    "    'advantage',\n",
    "    'risk',\n",
    "    'common_category_l1_x',\n",
    "    'common_category_l2_x',\n",
    "    'common_category_l3_x',\n",
    "    'easy_to_develop_reason',\n",
    "    'easy_to_develop_conclusion',\n",
    "    # 'material_reason',\n",
    "    'material_conclusion',\n",
    "    'm1_mid',\n",
    "    'm2_avg',\n",
    "    'm2_mid',\n",
    "    'm2_max',\n",
    "    'search_rank',\n",
    "    'purchace',\n",
    "    'products',\n",
    "    'ad_products',\n",
    "    'supply_demand_ratio',\n",
    "    'ad_supply_demand_ratio',\n",
    "    'monopoly_asin_dtos_0_click_rate',\n",
    "    'monopoly_asin_dtos_0_purchase_rate',\n",
    "    'monopoly_asin_dtos_1_click_rate',\n",
    "    'monopoly_asin_dtos_1_purchase_rate',\n",
    "    'monopoly_asin_dtos_2_click_rate',\n",
    "    'monopoly_asin_dtos_2_purchase_rate',\n",
    "    'monopoly_click_sum',\n",
    "    'monopoly_conversion_sum',\n",
    "    'spr',\n",
    "    'price_avg',\n",
    "    'reviews_avg',\n",
    "    'rating_avg',\n",
    "    'relative_available_date_days_avg',\n",
    "    'relative_available_date_days_mid',\n",
    "    'relative_available_date_days_min',\n",
    "    'relative_available_date_days_max',\n",
    "    'weight_mid',\n",
    "    'volume_mid',\n",
    "    'top3_sells',\n",
    "    'top3_amount',\n",
    "]\n",
    "# 只保留merged_df中top5_images列的第一个URL\n",
    "merged_df['top5_images'] = merged_df['top5_images_x'].apply(lambda x: x.split(';;')[0] if pd.notna(x) else x)\n",
    "# 将top3_sells拆分为三列\n",
    "sells_split = merged_df['top3_sells'].str.split(';;', expand=True)\n",
    "merged_df['top1_sells'] = sells_split[0]\n",
    "merged_df['top2_sells'] = sells_split[1] \n",
    "merged_df['top3_sells'] = sells_split[2]\n",
    "\n",
    "# 更新keep_cols列表\n",
    "keep_cols.remove('top3_sells')\n",
    "keep_cols.extend(['top1_sells', 'top2_sells', 'top3_sells'])\n",
    "merged_df[merged_df.score >= 5.5][keep_cols].to_csv('data/0913_part_1_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5ce2af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qk/mf0qlk8d141cxwxc748q0bd00000gn/T/ipykernel_35779/2771167973.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df = merged_df.groupby('common_category_l3_x').apply(\n"
     ]
    }
   ],
   "source": [
    "# # 对每个三级品类，保留score最高的前40%的数据\n",
    "# # 添加检查步骤确保过滤后的数据量更小\n",
    "# original_size = len(merged_df)\n",
    "# filtered_df = merged_df.groupby('common_category_l3_x').apply(\n",
    "#     lambda x: x.nlargest(int(len(x)), 'score')\n",
    "# ).reset_index(drop=True)[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e6fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
